{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import torch"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[0, 2781, 15902, 90348, 11332, 2]\n"
     ]
    }
   ],
   "source": [
    "from transformers import AutoModel, AutoTokenizer\n",
    "AUTH_TOKEN = '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'\n",
    "tokenizer = AutoTokenizer.from_pretrained('nguyenvulebinh/vi-mrc-base', use_auth_token='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')\n",
    "print(tokenizer.encode(\"thành phố hà nội\"))\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>text</th>\n",
       "      <th>queries</th>\n",
       "      <th>label</th>\n",
       "      <th>data_types</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>phạm văn đồng ( 1 tháng 3 năm 1906 – 29 tháng ...</td>\n",
       "      <td>Tên gọi nào được Phạm Văn Đồng sử dụng khi làm...</td>\n",
       "      <td>1</td>\n",
       "      <td>train</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>phạm văn đồng có vợ là bà phạm thị cúc và một ...</td>\n",
       "      <td>Tên gọi nào được Phạm Văn Đồng sử dụng khi làm...</td>\n",
       "      <td>0</td>\n",
       "      <td>train</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>ông việt phương , nguyên thư_ký của thủ_tướng ...</td>\n",
       "      <td>Tên gọi nào được Phạm Văn Đồng sử dụng khi làm...</td>\n",
       "      <td>0</td>\n",
       "      <td>train</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>bình định là mảnh đất có bề dày lịch_sử với nề...</td>\n",
       "      <td>Tên gọi nào được Phạm Văn Đồng sử dụng khi làm...</td>\n",
       "      <td>0</td>\n",
       "      <td>train</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>đầu năm 1126 , triều_kim của người nữ chân đã ...</td>\n",
       "      <td>Tên gọi nào được Phạm Văn Đồng sử dụng khi làm...</td>\n",
       "      <td>0</td>\n",
       "      <td>train</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                                text  \\\n",
       "0  phạm văn đồng ( 1 tháng 3 năm 1906 – 29 tháng ...   \n",
       "1  phạm văn đồng có vợ là bà phạm thị cúc và một ...   \n",
       "2  ông việt phương , nguyên thư_ký của thủ_tướng ...   \n",
       "3  bình định là mảnh đất có bề dày lịch_sử với nề...   \n",
       "4  đầu năm 1126 , triều_kim của người nữ chân đã ...   \n",
       "\n",
       "                                             queries  label data_types  \n",
       "0  Tên gọi nào được Phạm Văn Đồng sử dụng khi làm...      1      train  \n",
       "1  Tên gọi nào được Phạm Văn Đồng sử dụng khi làm...      0      train  \n",
       "2  Tên gọi nào được Phạm Văn Đồng sử dụng khi làm...      0      train  \n",
       "3  Tên gọi nào được Phạm Văn Đồng sử dụng khi làm...      0      train  \n",
       "4  Tên gọi nào được Phạm Văn Đồng sử dụng khi làm...      0      train  "
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df = pd.read_csv('train_ranking.csv')\n",
    "df.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Custom Datasest"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "from torch.utils.data import Dataset\n",
    "\n",
    "class SiameseDataset(Dataset):\n",
    "\n",
    "    def __init__(self, df, tokenizer, max_length, is_train):\n",
    "        if is_train:\n",
    "            self.df = df[df.data_types=='train']\n",
    "        else:\n",
    "            print('is_train=',is_train)\n",
    "            self.df = df[df.data_types=='test']\n",
    "\n",
    "        self.max_length = max_length\n",
    "        self.tokenizer = tokenizer\n",
    "        self.content1 = tokenizer.batch_encode_plus(list(self.df.queries.apply(lambda x: x.replace(\"_\",\" \")).values), max_length=max_length, truncation=True)[\"input_ids\"]\n",
    "        self.content2 = tokenizer.batch_encode_plus(list(self.df.text.apply(lambda x: x.replace(\"_\",\" \")).values), max_length=max_length, truncation=True)[\"input_ids\"]\n",
    "        self.targets = self.df.label.values\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.df)\n",
    "\n",
    "    def __getitem__(self, index):\n",
    "        return {\n",
    "            'ids1': torch.tensor(self.content1[index], dtype=torch.long),\n",
    "            'ids2': torch.tensor(self.content2[index][1:], dtype=torch.long),\n",
    "            'target': torch.tensor(self.targets[index], dtype=torch.float)\n",
    "        }\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch.nn as nn\n",
    "from transformers import AutoModel, AutoConfig\n",
    "\n",
    "class MeanPooling(nn.Module):\n",
    "    def __init__(self):\n",
    "        super(MeanPooling, self).__init__()\n",
    "\n",
    "    def forward(self, last_hidden_state, attention_mask):\n",
    "        input_mask_expanded = attention_mask.unsqueeze(-1).expand(last_hidden_state.size()).float()\n",
    "        sum_embeddings = torch.sum(last_hidden_state * input_mask_expanded, 1)\n",
    "        sum_mask = input_mask_expanded.sum(1)\n",
    "        sum_mask = torch.clamp(sum_mask, min=1e-9)\n",
    "        mean_embeddings = sum_embeddings / sum_mask\n",
    "        return mean_embeddings\n",
    "\n",
    "class PairwiseModel(nn.Module):\n",
    "    def __init__(self, model_name):\n",
    "        super(PairwiseModel, self).__init__()\n",
    "        self.model = AutoModel.from_pretrained(model_name, use_auth_token=AUTH_TOKEN)\n",
    "        self.config = AutoConfig.from_pretrained(model_name, use_auth_token=AUTH_TOKEN)\n",
    "        self.drop = nn.Dropout(p=0.2)\n",
    "        self.fc = nn.Linear(768, 1)\n",
    "        \n",
    "    def forward(self, ids, masks):\n",
    "        out = self.model(input_ids=ids,\n",
    "                           attention_mask=masks,\n",
    "                           output_hidden_states=False).last_hidden_state\n",
    "        out = out[:,0]\n",
    "        outputs = self.fc(out)\n",
    "        return outputs\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Training"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "pad_token_id = tokenizer.pad_token_id\n",
    "def collate_fn(batch):\n",
    "    ids = [torch.cat([x[\"ids1\"], x[\"ids2\"]]) for x in batch]\n",
    "    targets = [x[\"target\"] for x in batch]\n",
    "    max_len = np.max([len(x) for x in ids])\n",
    "    masks = []\n",
    "    for i in range(len(ids)):\n",
    "        if len(ids[i]) < max_len:\n",
    "            ids[i]= torch.cat((ids[i], torch.tensor([pad_token_id,]*(max_len - len(ids[i])),dtype=torch.long)))\n",
    "        masks.append(ids[i] != pad_token_id)\n",
    "    # print(tokenizer.decode(ids[0]))\n",
    "    outputs = {\n",
    "        \"ids\": torch.vstack(ids),\n",
    "        \"masks\": torch.vstack(masks),\n",
    "        \"target\": torch.vstack(targets).view(-1)\n",
    "    }\n",
    "    return outputs\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "def optimizer_scheduler(model, num_train_steps):\n",
    "    param_optimizer = list(model.named_parameters())\n",
    "    no_decay = [\"bias\", \"LayerNorm.weight\"]\n",
    "    optimizer_parameters = [\n",
    "            {\n",
    "                \"params\": [p for n, p in param_optimizer if not any(nd in n for nd in no_decay)],\n",
    "                \"weight_decay\": 0.001,\n",
    "            },\n",
    "            {\n",
    "                \"params\": [p for n, p in param_optimizer if any(nd in n for nd in no_decay)],\n",
    "                \"weight_decay\": 0.0,\n",
    "            },\n",
    "        ]\n",
    "\n",
    "    opt = AdamW(optimizer_parameters, lr=3e-5)\n",
    "    sch = get_linear_schedule_with_warmup(\n",
    "        opt,\n",
    "        num_warmup_steps=int(0.05*num_train_steps),\n",
    "        num_training_steps=num_train_steps,\n",
    "        last_epoch=-1,\n",
    "    )\n",
    "    return opt, sch\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of the model checkpoint at nguyenvulebinh/vi-mrc-base were not used when initializing RobertaModel: ['qa_outputs.weight', 'qa_outputs.bias']\n",
      "- This IS expected if you are initializing RobertaModel from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n",
      "- This IS NOT expected if you are initializing RobertaModel from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n",
      "Some weights of RobertaModel were not initialized from the model checkpoint at nguyenvulebinh/vi-mrc-base and are newly initialized: ['roberta.pooler.dense.bias', 'roberta.pooler.dense.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "is_train= False\n",
      "Done load dataset\n"
     ]
    }
   ],
   "source": [
    "from tqdm.auto import tqdm\n",
    "from torch.utils.data import DataLoader\n",
    "from sklearn.metrics import *\n",
    "\n",
    "loss_fn = nn.BCEWithLogitsLoss()\n",
    "epochs = 5\n",
    "accumulation_steps = 8\n",
    "scaler = torch.cuda.amp.GradScaler()\n",
    "error_ids = None\n",
    "\n",
    "model = PairwiseModel('nguyenvulebinh/vi-mrc-base')\n",
    "# model.load_state_dict(torch.load(f\"./outputs/pairwise_v2.bin\"))\n",
    "model.cuda()\n",
    "\n",
    "\n",
    "train_dataset = SiameseDataset(df, tokenizer, 384, True)\n",
    "valid_dataset = SiameseDataset(df, tokenizer, 384, False)\n",
    "train_loader = DataLoader(train_dataset, batch_size=8, collate_fn=collate_fn,\n",
    "                            num_workers=2, shuffle=True, pin_memory=True, drop_last=True)\n",
    "valid_loader = DataLoader(valid_dataset, batch_size=32, collate_fn=collate_fn,\n",
    "                            num_workers=2, shuffle=False, pin_memory=True)\n",
    "\n",
    "print('Done load dataset')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "from transformers import AutoTokenizer, AdamW, get_linear_schedule_with_warmup\n",
    "import numpy as np"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/opt/conda/lib/python3.8/site-packages/transformers/optimization.py:306: FutureWarning: This implementation of AdamW is deprecated and will be removed in a future version. Use the PyTorch implementation torch.optim.AdamW instead, or set `no_deprecation_warning=True` to disable this warning\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "13d93f90f9b249559d7089402e4ba7d3",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/5 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
      "To disable this warning, you can either:\n",
      "\t- Avoid using `tokenizers` before the fork if possible\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n",
      "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
      "To disable this warning, you can either:\n",
      "\t- Avoid using `tokenizers` before the fork if possible\n",
      "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "7f074f34d8334f4ebc812343b9fcf93a",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/28685 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "num_train_steps = len(train_loader) * epochs // accumulation_steps\n",
    "optimizer, scheduler = optimizer_scheduler(model, num_train_steps)\n",
    "\n",
    "for epoch in tqdm(range(epochs)):\n",
    "    model.train()\n",
    "    bar = tqdm(enumerate(train_loader), total=len(train_loader), leave=False)\n",
    "    for step, data in bar:\n",
    "        ids = data[\"ids\"].cuda()\n",
    "        # for x in ids:\n",
    "        #     print(tokenizer.decode(x))\n",
    "        masks = data[\"masks\"].cuda()\n",
    "        target = data[\"target\"].cuda()\n",
    "        # with torch.cuda.amp.autocast():\n",
    "        preds = model(ids, masks)\n",
    "        # print(preds.view(-1))\n",
    "        loss = loss_fn(preds.view(-1), target.view(-1))\n",
    "        loss /= accumulation_steps\n",
    "        loss.backward()\n",
    "        if (step + 1) % accumulation_steps == 0:\n",
    "            optimizer.step()\n",
    "            # scaler.update()\n",
    "            optimizer.zero_grad()\n",
    "            scheduler.step()\n",
    "        bar.set_postfix(loss=loss.item())\n",
    "\n",
    "    model.eval()\n",
    "    with torch.no_grad():\n",
    "        bar = tqdm(enumerate(valid_loader), total=len(valid_loader), leave=False)\n",
    "        targets = []\n",
    "        all_preds = []\n",
    "        for step, data in bar:\n",
    "            ids = data[\"ids\"].cuda()\n",
    "            masks = data[\"masks\"].cuda()\n",
    "            target = data[\"target\"].cuda()\n",
    "            preds = torch.sigmoid(model(ids, masks))\n",
    "            all_preds.extend(preds.cpu().view(-1).numpy())\n",
    "            targets.extend(target.cpu().view(-1).numpy())\n",
    "        all_preds = np.array(all_preds)\n",
    "        targets = np.array(targets)\n",
    "\n",
    "        print(f\"F1 {f1_score(targets, all_preds > 0.5)}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "F1 0.7832579185520362\n"
     ]
    }
   ],
   "source": [
    "print(f\"F1 {recall_score(np.array(targets), np.array(all_preds) > 0.5)}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "torch.save(model.state_dict(), f\"./pairwise_v2.bin\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of the model checkpoint at nguyenvulebinh/vi-mrc-base were not used when initializing RobertaModel: ['qa_outputs.weight', 'qa_outputs.bias']\n",
      "- This IS expected if you are initializing RobertaModel from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n",
      "- This IS NOT expected if you are initializing RobertaModel from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n",
      "Some weights of RobertaModel were not initialized from the model checkpoint at nguyenvulebinh/vi-mrc-base and are newly initialized: ['roberta.pooler.dense.weight', 'roberta.pooler.dense.bias']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<All keys matched successfully>"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model1 = PairwiseModel('nguyenvulebinh/vi-mrc-base')\n",
    "model1.load_state_dict(torch.load(f\"./pairwise_v2.bin\"))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# from transformers import *\n",
    "\n",
    "# class MonoBERT(BertPreTrainedModel):\n",
    "#     def __init__(self, config):\n",
    "#         config.num_labels = 1\n",
    "#         super(MonoBERT, self).__init__(config)\n",
    "#         self.bert = BertForSequenceClassification(config)\n",
    "#         self.init_weights()\n",
    "\n",
    "#     def forward(self, input_ids, attention_mask, token_type_ids):\n",
    "#         outputs = self.bert(input_ids, attention_mask, token_type_ids)\n",
    "#         logits = outputs[0]\n",
    "#         return logits\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# import torch\n",
    "# from torch.nn.functional import cross_entropy\n",
    "# from transformers import AdamW\n",
    "\n",
    "# model = MonoBERT.from_pretrained(\"bert-base-uncased\")\n",
    "# tokenizer = BertTokenizer.from_pretrained(\"bert-base-uncased\")\n",
    "\n",
    "# optimizer = AdamW(model.parameters(), lr=1e-5, eps=1e-8)\n",
    "# optimizer.zero_grad()\n",
    "\n",
    "# pos_text = \"{} [SEP] {}\".format(query, pos_doc) # query, pos_doc and neg_doc can be \n",
    "# neg_text = \"{} [SEP] {}\".format(query, neg_doc) /#retrieved from the training triples\n",
    "\n",
    "# pos_encoded = tokenizer.encode_plus(pos_text, return_tensors=\"pt\")\n",
    "# neg_encoded = tokenizer.encode_plus(neg_text, return_tensors=\"pt\")\n",
    "\n",
    "# pos_output = model.forward(**pos_encoded).squeeze(1)\n",
    "# neg_output = model.forward(**neg_encoded).squeeze(1)\n",
    "\n",
    "# labels = torch.zeros(1, dtype=torch.long)\n",
    "# loss = cross_entropy(torch.stack((pos_output, neg_output), dim=1), labels)\n",
    "\n",
    "# loss.backward()\n",
    "# optimizer.step()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Bert"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# from transformers import AutoModel, AutoTokenizer\n",
    "# import torch\n",
    "# import pytorch_lightning as pl\n",
    "# import torch_optimizer as optim\n",
    "# from transformers import (BertForNextSentencePrediction, BertModel, get_linear_schedule_with_warmup)\n",
    "# import torch.distributed as dist\n",
    "# from torch import nn\n",
    "# import pytrec_eval\n",
    "# import gc\n",
    "\n",
    "\n",
    "# class CrossEncoder(torch.nn.Module):\n",
    "#     def __init__(self,\n",
    "#                  encoder_name_or_dir,\n",
    "#                  encoder_config=None,\n",
    "#                  cache_dir=None):\n",
    "#         super().__init__()\n",
    "#         self.encoder = BertForNextSentencePrediction.from_pretrained(encoder_name_or_dir,\n",
    "#                                                                      config=encoder_config,\n",
    "#                                                                      cache_dir=cache_dir)\n",
    "\n",
    "#     def forward(self, inputs, labels=None):\n",
    "#         outputs = self.encoder(**inputs, labels=labels)\n",
    "#         return outputs\n",
    "\n",
    "\n",
    "# class BertReranker(pl.LightningModule):\n",
    "#     def __init__(self,\n",
    "#                  encoder_name_or_dir,\n",
    "#                  encoder_config=None,\n",
    "#                  cache_dir=None,\n",
    "#                  optimizer=\"adam\",\n",
    "#                  lr=1e-5,\n",
    "#                  warm_up_steps=1700,\n",
    "#                  num_gpus=1,\n",
    "#                  batch_size=64,\n",
    "#                  num_epochs=2,\n",
    "#                  train_set_size=532761,  # ms marco train size\n",
    "#                  num_neg_per_pos=4\n",
    "#                  ):\n",
    "#         super().__init__()\n",
    "\n",
    "#         self.save_hyperparameters()\n",
    "\n",
    "#         self.encoder = CrossEncoder(encoder_name_or_dir,\n",
    "#                                     encoder_config,\n",
    "#                                     cache_dir)\n",
    "\n",
    "#     def training_step(self, batch, batch_idx):\n",
    "#         inputs, labels = batch\n",
    "#         outputs = self.encoder(inputs, labels=labels)\n",
    "#         loss = outputs.loss\n",
    "#         self.log(\"train_loss\", loss.item())\n",
    "#         return loss\n",
    "\n",
    "#     def forward(self, inputs):\n",
    "#         outputs = self.encoder(inputs)\n",
    "#         return outputs\n",
    "\n",
    "#     def get_scores(self, inputs):\n",
    "#         outputs = self.encoder(inputs)\n",
    "#         logits = outputs.logits\n",
    "#         scores = torch.softmax(logits, dim=1)[:, 1]\n",
    "\n",
    "#         return scores\n",
    "\n",
    "#     def configure_optimizers(self):\n",
    "#         optimizer = None\n",
    "#         lr = self.hparams.lr\n",
    "#         if self.hparams.optimizer == 'adam':\n",
    "#             optimizer = torch.optim.Adam(self.parameters(), lr=lr)\n",
    "\n",
    "#         if self.hparams.optimizer == 'lamb':\n",
    "#             optimizer = optim.Lamb(self.parameters(), lr=lr)\n",
    "#         total_steps = self.hparams.num_epochs * \\\n",
    "#                       int(self.hparams.train_set_size / (self.hparams.batch_size * self.hparams.num_gpus))\n",
    "\n",
    "#         # def lr_lambda(current_step):\n",
    "#         #     if current_step < self.hparams.warm_up_step:\n",
    "#         #         lr_scale = 0.1 * (current_step/self.hparams.warm_up_step)\n",
    "#         #     else:\n",
    "#         #         lr_scale = 0.1 * (0.90 ** (current_step - self.hparams.warm_up_step))\n",
    "#         #         if lr_scale < self.hparams.lr:\n",
    "#         #             lr_scale = self.hparams.lr\n",
    "#         #     return lr_scale\n",
    "#         #\n",
    "#         # scheduler = torch.optim.lr_scheduler.LambdaLR(\n",
    "#         #     optimizer,\n",
    "#         #     lr_lambda=lr_lambda,\n",
    "#         # )\n",
    "\n",
    "#         if self.hparams.warm_up_steps == 0:\n",
    "#             return optimizer\n",
    "\n",
    "#         scheduler = get_linear_schedule_with_warmup(\n",
    "#             optimizer, num_warmup_steps=self.hparams.warm_up_steps, num_training_steps=total_steps\n",
    "#         )\n",
    "#         schedulers = [{\n",
    "#             'scheduler': scheduler,\n",
    "#             'name': 'warm_up_lr',\n",
    "#             'interval': 'step'\n",
    "#         }]\n",
    "#         optimizers = [optimizer]\n",
    "#         return optimizers, schedulers"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
