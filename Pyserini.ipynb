{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "3b12ea42-61b4-482c-8b01-9182cb27c577",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import json\n",
    "from utils import preprocess\n",
    "from tqdm.notebook import trange, tqdm\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "143fe609-2620-46db-b59e-f861b635891a",
   "metadata": {},
   "source": [
    "## Load data to Index"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "24b4a7f6",
   "metadata": {},
   "outputs": [],
   "source": [
    "f = open('ViQuAD/train_ViQuAD.json', 'r')\n",
    "train_set = json.load(f)\n",
    "train_set = train_set['data']\n",
    "f = open('ViQuAD/dev_ViQuAD.json', 'r')\n",
    "dev_set = json.load(f)\n",
    "dev_set = dev_set['data']\n",
    "f = open('ViQuAD/test_ViQuAD.json', 'r')\n",
    "test_set = json.load(f)\n",
    "test_set = test_set['data']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "71b3f124",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "5109"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "contexts = []\n",
    "questions = []\n",
    "id = 0\n",
    "all_data = train_set+dev_set+test_set\n",
    "for article in all_data:\n",
    "    title = article['title']\n",
    "    for paragraph in article['paragraphs']:\n",
    "        context = preprocess(paragraph['context'])\n",
    "        contexts.append(\n",
    "            {'id': id,\n",
    "            'contents': context,\n",
    "            'title': title\n",
    "        })\n",
    "        for q in paragraph['qas']:\n",
    "            questions.append({'answers': context,\n",
    "                             'question': q['question']})\n",
    "        id+=1\n",
    "len(contexts)\n",
    "# with open('collections/documents.json','w') as f:\n",
    "#     json.dump(contexts, f)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "3b89cb33-0ed1-4378-a2c1-82dae4d449b4",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "df_wiki_windows = pd.read_csv(\"/mmlabworkspace/Students/AIC/old/zalo/hungcv/temp/wikipedia_20220620_cleaned_v2.csv\")\n",
    "df_wiki = pd.read_csv(\"/mmlabworkspace/Students/AIC/old/zalo/hungcv/data/wikipedia_20220620_short.csv\")\n",
    "df_wiki.title = df_wiki.title.apply(str)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "d654eab9",
   "metadata": {},
   "outputs": [],
   "source": [
    "corpus = df_wiki_windows['bm25_text'].values"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "6629dee1",
   "metadata": {},
   "outputs": [],
   "source": [
    "f=open('./doc2query/doc2query.json')\n",
    "data = json.load(f)\n",
    "\n",
    "id = 5109\n",
    "for i in corpus:\n",
    "    data.append(\n",
    "        {'id': id,\n",
    "        'contents': i,\n",
    "        'title': 'wiki',\n",
    "        'ori_contents': i\n",
    "\n",
    "    })\n",
    "\n",
    "    id+=1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "8701c1e4",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'id': 0,\n",
       " 'contents': 'phạm văn đồng ( 1 tháng 3 năm 1906 – 29 tháng 4 năm 2000 ) là thủ_tướng đầu_tiên của nước cộng_hòa xã_hội chủ_nghĩa việt_nam từ năm 1976 ( từ năm 1981 gọi là chủ_tịch hội_đồng_bộ_trưởng ) cho đến khi nghỉ hưu năm 1987 . trước đó ông từng giữ chức_vụ thủ_tướng chính_phủ việt nam dân_chủ cộng_hòa từ năm 1955 đến năm 1976 . ông là vị thủ_tướng việt_nam tại vị lâu nhất ( 1955 – 1987 ) . ông là học_trò , cộng_sự của chủ_tịch hồ chí minh . ông có tên gọi thân_mật là tô , đây từng là bí_danh của ông . ông còn có tên gọi là lâm bá kiệt khi làm phó chủ_nhiệm cơ_quan biện sự xứ tại quế lâm ( chủ_nhiệm là hồ học lãm ) . tên của thủ_tướng việt nam là gì ai là chủ_tịch chính_phủ đầu_tiên của nước cộng_hòa xã_hội chủ_nghĩa việt nam thủ_tướng việt nam là ai thủ_tướng việt nam là ai phạm văn đồng là ai',\n",
       " 'title': 'Phạm Văn Đồng',\n",
       " 'ori_contents': 'phạm văn đồng ( 1 tháng 3 năm 1906 – 29 tháng 4 năm 2000 ) là thủ_tướng đầu_tiên của nước cộng_hòa xã_hội chủ_nghĩa việt_nam từ năm 1976 ( từ năm 1981 gọi là chủ_tịch hội_đồng_bộ_trưởng ) cho đến khi nghỉ hưu năm 1987 . trước đó ông từng giữ chức_vụ thủ_tướng chính_phủ việt nam dân_chủ cộng_hòa từ năm 1955 đến năm 1976 . ông là vị thủ_tướng việt_nam tại vị lâu nhất ( 1955 – 1987 ) . ông là học_trò , cộng_sự của chủ_tịch hồ chí minh . ông có tên gọi thân_mật là tô , đây từng là bí_danh của ông . ông còn có tên gọi là lâm bá kiệt khi làm phó chủ_nhiệm cơ_quan biện sự xứ tại quế lâm ( chủ_nhiệm là hồ học lãm ) .'}"
      ]
     },
     "execution_count": 22,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "data[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "17efe723",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "mkdir: cannot create directory ‘full_wiki’: File exists\n"
     ]
    }
   ],
   "source": [
    "!mkdir full_wiki\n",
    "with open('full_wiki/documents.json','w') as f:\n",
    "    json.dump(data, f)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7ecab744-fc4c-4996-864c-d7cb25c88aca",
   "metadata": {},
   "source": [
    "## Index"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7d9e1020",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "!python -m pyserini.index.lucene \\\n",
    "    --collection JsonCollection \\\n",
    "    --input full_wiki \\\n",
    "    --index indexes/full_wiki \\\n",
    "    --generator DefaultLuceneDocumentGenerator \\\n",
    "    --threads 4 \\\n",
    "    --storePositions --storeDocvectors --storeRaw"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "64ee9bd5-e0ce-4241-84df-e3c6e9558922",
   "metadata": {},
   "source": [
    "## test searcher"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "id": "7bbeb05d",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of the model checkpoint at nguyenvulebinh/vi-mrc-base were not used when initializing RobertaModel: ['qa_outputs.weight', 'qa_outputs.bias']\n",
      "- This IS expected if you are initializing RobertaModel from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n",
      "- This IS NOT expected if you are initializing RobertaModel from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n"
     ]
    }
   ],
   "source": [
    "from retrieval import Pyserini_Search\n",
    "searcher = Pyserini_Search(index_path='indexes/full_wiki')\n",
    "# searcher.search(\"Cộng hòa Weimar chính thức thay thế đế quốc Đức kể từ sau sự kiện nào?\", 100)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e000360d-bfee-42e2-8b9a-c6d48261ab7d",
   "metadata": {},
   "source": [
    "## Evaluate"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "id": "04005c38-6da7-433a-b637-dfa3842d44eb",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "91d556c0d191497dace647d515ddf5b9",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "HBox(children=(HTML(value=''), FloatProgress(value=0.0, max=2210.0), HTML(value='')))"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "## Read data from test_set\n",
    "questions = []\n",
    "id = 0\n",
    "for article in test_set:\n",
    "    title = article['title']\n",
    "    for paragraph in article['paragraphs']:\n",
    "        for q in paragraph['qas']:\n",
    "            questions.append({'id': q['id'],\n",
    "                              'gt_contexts': preprocess(paragraph['context']),\n",
    "                             'question': q['question'],\n",
    "                             'answers': q['answers'],\n",
    "                             'title': title\n",
    "                             })\n",
    "        id+=1\n",
    "\n",
    "for i in trange(len(questions)):\n",
    "    contexts, ranking_scores = searcher.search(questions[i]['question'],use_rerank=True, topk=100)\n",
    "    questions[i]['contexts'] = contexts"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "id": "9599875d-ba0c-47f1-9bf7-b083f70ff90d",
   "metadata": {},
   "outputs": [],
   "source": [
    "def evaluate(questions, topk=[1,5,10], max_k=100):\n",
    "    accuracy = { k : [] for k in topk }\n",
    "    out = []\n",
    "    for question in tqdm(questions):\n",
    "        answers = question['gt_contexts']    #ddasp an\n",
    "        contexts = question['contexts']  #search ra\n",
    "        has_ans_idx = max_k  # first index in contexts that has answers\n",
    "        for idx, ctx in enumerate(contexts):\n",
    "            if idx >= max_k:\n",
    "                break\n",
    "            if ctx==answers:\n",
    "                has_ans_idx = idx\n",
    "                out.append(has_ans_idx)\n",
    "                break\n",
    "        for k in topk:\n",
    "            accuracy[k].append(0 if has_ans_idx >= k else 1)\n",
    "            \n",
    "    for k in topk:\n",
    "        print(f'Top{k}\\taccuracy: {np.mean(accuracy[k]):.4f}')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "id": "b59aca28-75eb-4448-ac29-2815d26866d6",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "c4c84d1e7d1f48a79ab217ee242d2f24",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "HBox(children=(HTML(value=''), FloatProgress(value=0.0, max=2210.0), HTML(value='')))"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Top1\taccuracy: 0.6516\n",
      "Top5\taccuracy: 0.8213\n",
      "Top10\taccuracy: 0.8357\n",
      "Top15\taccuracy: 0.8575\n",
      "Top20\taccuracy: 0.8787\n",
      "Top25\taccuracy: 0.8937\n",
      "Top30\taccuracy: 0.9023\n"
     ]
    }
   ],
   "source": [
    "evaluate(questions, topk=[1,5,10, 15, 20, 25, 30], max_k=100)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "65fe037d",
   "metadata": {},
   "source": [
    "## Create ranking dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "id": "ec59cac8",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "138"
      ]
     },
     "execution_count": 51,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(train_set+dev_set)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "id": "1f1851fa",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "78a124c7c360434d9599b3de523adbd9",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "HBox(children=(HTML(value=''), FloatProgress(value=0.0, max=23074.0), HTML(value='')))"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "questions = []\n",
    "id = 0\n",
    "all_data = train_set+dev_set+test_set\n",
    "for article in all_data:\n",
    "    title = article['title']\n",
    "    for paragraph in article['paragraphs']:\n",
    "        for q in paragraph['qas']:\n",
    "            questions.append({'answers': preprocess(paragraph['context']),\n",
    "                            'question': q['question']})\n",
    "        id+=1\n",
    "\n",
    "from tqdm.notebook import trange, tqdm\n",
    "for i in trange(len(questions)):\n",
    "    hits = searcher.search(preprocess(questions[i]['question']),k=100)\n",
    "    contexts = []\n",
    "    questions[i]['contexts'] = [json.loads(hits[j].raw)['contents'] for j in range(min(len(hits),100))]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 63,
   "id": "c72d5694",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "0d0a9a91be9441ce8ac00c45c863e820",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "HBox(children=(HTML(value=''), FloatProgress(value=1.0, bar_style='info', layout=Layout(width='20px'), max=1.0…"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "texts = []\n",
    "queries = []\n",
    "labels = []\n",
    "data_types= []\n",
    "\n",
    "test_index = 0\n",
    "for article in (train_set+dev_set):\n",
    "    for paragraph in article['paragraphs']:\n",
    "        test_index += len(paragraph['qas'])\n",
    "\n",
    "for index, question in tqdm(enumerate(questions)):\n",
    "    neg, pos = retrievaler.sample( question['question'], question['answers'], 11, max_query_len = 512)\n",
    "    a = question['question']\n",
    "\n",
    "    # assert len(neg) == 50, f'len(neg)={len(neg)} should be 50'\n",
    "    samples = len(neg) +1\n",
    "    \n",
    "    texts.append(pos)\n",
    "    labels.append(1)    \n",
    "    texts.extend(neg)\n",
    "    labels.extend([0]*(samples-1))\n",
    "    queries.extend([question['question']]*samples)\n",
    "\n",
    "    if index < test_index:\n",
    "        data_types.extend(['train']*samples)\n",
    "    else:\n",
    "        data_types.extend(['test']*samples)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 64,
   "id": "022ea6fa",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "df = pd.DataFrame()\n",
    "df[\"text\"] = texts\n",
    "df[\"queries\"] = queries\n",
    "df[\"label\"] = labels\n",
    "df['data_types'] = data_types"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 68,
   "id": "6f347ae8",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>text</th>\n",
       "      <th>queries</th>\n",
       "      <th>label</th>\n",
       "      <th>data_types</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>phạm văn đồng ( 1 tháng 3 năm 1906 – 29 tháng ...</td>\n",
       "      <td>Tên gọi nào được Phạm Văn Đồng sử dụng khi làm...</td>\n",
       "      <td>1</td>\n",
       "      <td>train</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>phạm văn đồng có vợ là bà phạm thị cúc và một ...</td>\n",
       "      <td>Tên gọi nào được Phạm Văn Đồng sử dụng khi làm...</td>\n",
       "      <td>0</td>\n",
       "      <td>train</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>ông việt phương , nguyên thư_ký của thủ_tướng ...</td>\n",
       "      <td>Tên gọi nào được Phạm Văn Đồng sử dụng khi làm...</td>\n",
       "      <td>0</td>\n",
       "      <td>train</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>bình định là mảnh đất có bề dày lịch_sử với nề...</td>\n",
       "      <td>Tên gọi nào được Phạm Văn Đồng sử dụng khi làm...</td>\n",
       "      <td>0</td>\n",
       "      <td>train</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>đầu năm 1126 , triều_kim của người nữ chân đã ...</td>\n",
       "      <td>Tên gọi nào được Phạm Văn Đồng sử dụng khi làm...</td>\n",
       "      <td>0</td>\n",
       "      <td>train</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>229479</th>\n",
       "      <td>uganda tuyên_bố đi theo đường_lối trung_lập , ...</td>\n",
       "      <td>Hiện nay, kinh tế Uganda cần phải tập trung ph...</td>\n",
       "      <td>0</td>\n",
       "      <td>train</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>229480</th>\n",
       "      <td>từ ngàn năm nay , việt nam là một nước nông_ng...</td>\n",
       "      <td>Hiện nay, kinh tế Uganda cần phải tập trung ph...</td>\n",
       "      <td>0</td>\n",
       "      <td>train</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>229481</th>\n",
       "      <td>hoạt_động_kinh_tế ở khu_vực paris cũng đa_dạng...</td>\n",
       "      <td>Hiện nay, kinh tế Uganda cần phải tập trung ph...</td>\n",
       "      <td>0</td>\n",
       "      <td>train</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>229482</th>\n",
       "      <td>bất_kỳ tác vụ nào có_thể phá_hủy dữ_liệu của m...</td>\n",
       "      <td>Hiện nay, kinh tế Uganda cần phải tập trung ph...</td>\n",
       "      <td>0</td>\n",
       "      <td>train</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>229483</th>\n",
       "      <td>tuy được pháp gọi là \" hòn ngọc viễn đông \" nh...</td>\n",
       "      <td>Hiện nay, kinh tế Uganda cần phải tập trung ph...</td>\n",
       "      <td>0</td>\n",
       "      <td>train</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>229484 rows × 4 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "                                                     text  \\\n",
       "0       phạm văn đồng ( 1 tháng 3 năm 1906 – 29 tháng ...   \n",
       "1       phạm văn đồng có vợ là bà phạm thị cúc và một ...   \n",
       "2       ông việt phương , nguyên thư_ký của thủ_tướng ...   \n",
       "3       bình định là mảnh đất có bề dày lịch_sử với nề...   \n",
       "4       đầu năm 1126 , triều_kim của người nữ chân đã ...   \n",
       "...                                                   ...   \n",
       "229479  uganda tuyên_bố đi theo đường_lối trung_lập , ...   \n",
       "229480  từ ngàn năm nay , việt nam là một nước nông_ng...   \n",
       "229481  hoạt_động_kinh_tế ở khu_vực paris cũng đa_dạng...   \n",
       "229482  bất_kỳ tác vụ nào có_thể phá_hủy dữ_liệu của m...   \n",
       "229483  tuy được pháp gọi là \" hòn ngọc viễn đông \" nh...   \n",
       "\n",
       "                                                  queries  label data_types  \n",
       "0       Tên gọi nào được Phạm Văn Đồng sử dụng khi làm...      1      train  \n",
       "1       Tên gọi nào được Phạm Văn Đồng sử dụng khi làm...      0      train  \n",
       "2       Tên gọi nào được Phạm Văn Đồng sử dụng khi làm...      0      train  \n",
       "3       Tên gọi nào được Phạm Văn Đồng sử dụng khi làm...      0      train  \n",
       "4       Tên gọi nào được Phạm Văn Đồng sử dụng khi làm...      0      train  \n",
       "...                                                   ...    ...        ...  \n",
       "229479  Hiện nay, kinh tế Uganda cần phải tập trung ph...      0      train  \n",
       "229480  Hiện nay, kinh tế Uganda cần phải tập trung ph...      0      train  \n",
       "229481  Hiện nay, kinh tế Uganda cần phải tập trung ph...      0      train  \n",
       "229482  Hiện nay, kinh tế Uganda cần phải tập trung ph...      0      train  \n",
       "229483  Hiện nay, kinh tế Uganda cần phải tập trung ph...      0      train  \n",
       "\n",
       "[229484 rows x 4 columns]"
      ]
     },
     "execution_count": 68,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df[df.data_types=='train']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 66,
   "id": "5af01d23",
   "metadata": {},
   "outputs": [],
   "source": [
    "df.to_csv('train_ranking.csv',index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 67,
   "id": "d178a0b5",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "text,queries,label,data_types\n",
      "\"phạm văn đồng ( 1 tháng 3 năm 1906 – 29 tháng 4 năm 2000 ) là thủ_tướng đầu_tiên của nước cộng_hòa xã_hội chủ_nghĩa việt_nam từ năm 1976 ( từ năm 1981 gọi là chủ_tịch hội_đồng_bộ_trưởng ) cho đến khi nghỉ hưu năm 1987 . trước đó ông từng giữ chức_vụ thủ_tướng chính_phủ việt nam dân_chủ cộng_hòa từ năm 1955 đến năm 1976 . ông là vị thủ_tướng việt_nam tại vị lâu nhất ( 1955 – 1987 ) . ông là học_trò , cộng_sự của chủ_tịch hồ chí minh . ông có tên gọi thân_mật là tô , đây từng là bí_danh của ông . ông còn có tên gọi là lâm bá kiệt khi làm phó chủ_nhiệm cơ_quan biện sự xứ tại quế lâm ( chủ_nhiệm là hồ học lãm ) .\",Tên gọi nào được Phạm Văn Đồng sử dụng khi làm Phó chủ nhiệm cơ quan Biện sự xứ tại Quế Lâm?,1,train\n",
      "\"phạm văn đồng có vợ là bà phạm thị cúc và một người con_trai duy_nhất tên là phạm sơn_dương , hiện là thiếu_tướng quân_đội nhân_dân việt_nam , phó_giám_đốc viện khoa_học và công_nghệ quân_sự . sau khi lấy bà cúc ( tháng 10 năm 1946 ) phạm văn đồng vào công_tác trong liên_khu 5 . mấy năm sau bà cúc được phép vào nam sống với chồng . vào đến nơi thì phạm văn đồng lại được lệnh ra bắc . sau đó bà cúc bị bệnh \"\" nửa quên nửa nhớ \"\" ( theo lời của việt phương , người từng làm thư_ký cho phạm văn đồng trong 53 năm ) kéo_dài cho đến tận bây_giờ . phạm văn đồng từng đưa bà cúc sang trung_quốc , liên xô chữa bệnh nhưng vẫn không khỏi . phạm văn đồng mất trước bà cúc .\",Tên gọi nào được Phạm Văn Đồng sử dụng khi làm Phó chủ nhiệm cơ quan Biện sự xứ tại Quế Lâm?,0,train\n",
      "\"ông việt phương , nguyên thư_ký của thủ_tướng phạm văn đồng , trong buổi họp_báo giới_thiệu sách của các nhà ngoại_giao , đã tiết_lộ rằng khi đàm_phán hiệp_định geneva ( 1954 ) , do đoàn việt nam không có điện_đài nên bộ_trưởng ngoại_giao lúc đó là phạm văn đồng đã mắc một sai_lầm khi nhờ trung quốc chuyển các bức điện về nước , do vậy trung quốc biết hết các sách_lược của việt_nam và sử_dụng chúng để ép việt_nam ký hiệp_định theo lợi_ích của trung quốc . trong đàm_phán phạm văn đồng sử_dụng phiên_dịch trung quốc nên nội_dung liên_lạc giữa đoàn đàm_phán và trung_ương , trung quốc đều biết trước và tìm cách ngăn_chặn . ông phạm văn đồng sau_này cũng thừa_nhận là đoàn việt_nam khi đó quá tin đoàn trung quốc . tại hội_nghị ấy , ông đồng chỉ chủ_yếu tiếp_xúc với đoàn liên xô và đoàn trung_quốc , trong khi anh là đồng chủ_tịch , quan_điểm lại khác với pháp , nhưng ông lại không tranh_thủ , không hề tiếp_xúc với phái_đoàn anh .\",Tên gọi nào được Phạm Văn Đồng sử dụng khi làm Phó chủ nhiệm cơ quan Biện sự xứ tại Quế Lâm?,0,train\n",
      "\"bình định là mảnh đất có bề dày lịch_sử với nền văn_hoá sa huỳnh , từng là cố_đô của vương_quốc chămpa mà di_sản còn lưu_giữ là thành đồ bàn và các tháp chàm với nghệ_thuật kiến_trúc độc_đáo . đây cũng là nơi xuất_phát phong_trào nông_dân khởi_nghĩa vào thế_kỷ 18 với tên_tuổi của anh_hùng áo vải nguyễn huệ ; là quê_hương của các danh_nhân_trần quang diệu , bùi thị xuân , nguyễn đăng lâm , bác_sĩ phạm ngọc_thạch , ngô mây , tăng bạt hổ , diệp trường phát ... , và các văn thi nhân nguyễn diêu , đào tấn , hàn mặc tử , xuân diệu , chế lan_viên , yến lan , quách tấn , nguyễn thành long , phạm hổ , phạm văn ký ... bình định còn được biết đến với truyền_thống thượng_võ và có nền văn_hoá đa_dạng phong_phú với các loại_hình nghệ_thuật như bài_chòi , hát_bội , nhạc võ tây_sơn , hò bá trạo của cư_dân vùng_biển ... cùng với các lễ_hội như : lễ_hội đống đa , lễ_hội cầu ngư , lễ_hội của các dân_tộc miền núi ...\",Tên gọi nào được Phạm Văn Đồng sử dụng khi làm Phó chủ nhiệm cơ quan Biện sự xứ tại Quế Lâm?,0,train\n",
      "\"đầu năm 1126 , triều_kim của người nữ chân đã cho quân vượt hoàng hà , tiến sát tới biện kinh . quá hoảng_sợ , tống huy tông đã thoái_vị ngày 18 tháng 1 năm 1126 để nhường ngôi cho kỳ tử triệu hoàn , tức tống khâm tông . quân kim sau đó bỏ việc bao_vây khai phong và quay trở về phương bắc song tống đã buộc phải ký hòa ước với nhà kim . tuy_nhiên , vài tháng sau , vào tháng 9 năm 1126 , quân kim lại quay trở_lại phương nam một lần nữa . quân kim cuối_cùng đã vào được biện kinh vào ngày 9 tháng 1 năm 1127 và tiến_hành các vụ cướp_bóc , hãm_hiếp , thảm_sát trong nhiều ngày liền . trong sự_kiện tĩnh khang , quân kim đã bắt huy tông , khâm tông , các tôn thất và bá quan của triều tống . bắc tống diệt_vong , song hoàng_tử triệu cấu đã kịp chạy trốn về miền nam , và thành_lập triều_đại nam tống . năm 1129 , kim thái tông hạ_lệnh cho tông phụ và ngột truật mang quân đánh triều_nam tống mới hình_thành . quân kim vượt hoàng hà rồi chia làm hai nhánh , ngột truật lĩnh 10 vạn quân đánh phủ khai đức ( nay thuộc huyện bộc_dương ) nhưng bị thiếu lương nên ông mang quân quay lại đánh bộc châu . ngột truật cử tướng tiên_phong ô lâm đáp thái ( 乌林答大 ) tiến lên trước , đại phá 20 vạn quân tống của vương thiện . hạ được bộc châu , ngột truật thừa_thắng chiếm luôn 5 huyện lân_cận . sau đó , ngột truật mang quân quay lại đánh khai đức . khi ngột truật dẫn quân kim tiến đến quy đức ( nay thuộc thương khâu ) , đã cho lính áp sát thành , đặt hỏa_pháo ngay trên bờ hào , quân tống trong thành sợ_hãi xin hàng . cuối_cùng , kim và tống lấy hoài hà làm ranh_giới , triều tống phải cống_nạp hằng năm cho kim . trong thời_gian này , do được hưởng trạng_thái hòa bình lâu_dài cùng sự thịnh_vượng về kinh_tế - văn_hóa , vùng giang nam đã thay_thế trung_nguyên để trở_thành trung_tâm kinh_tế - văn_hóa của trung quốc . thời thuộc kim , hà_nam về mặt hành_chính được phân thành nam kinh_lộ và hà đông nam lộ , kinh_triệu phủ lộ , đại_danh phủ lộ , hà bắc đông_lộ , còn về mặt kinh_tế là một trong các khu_vực kinh_tế phát_triển nhất nước . biện kinh đóng vai_trò là \"\" nam kinh \"\" của triều kim từ năm 1157 ( nguồn khác nói là từ năm 1161 ) và được xây_dựng lại trong thời_gian này . năm 1214 , sau khi mông - kim hòa nghị thành_công , kim tuyên tông đã thiên_đô đến biện kinh . sau dó , kim tiến_hành chiến_tranh chống nam tống kéo_dài từ năm 1217 tới đầu năm 1224 . kim bị tiêu_diệt vào năm 1234 trước sự tấn_công của liên_quân mông_cổ - nam tống .\",Tên gọi nào được Phạm Văn Đồng sử dụng khi làm Phó chủ nhiệm cơ quan Biện sự xứ tại Quế Lâm?,0,train\n",
      "\"trước tình_hình mâu_thuẫn pháp việt ngày_càng leo_thang , cédile nhờ thiếu_tá a . peter dewey , chỉ_huy oss tại sài_gòn , gặp các lãnh_đạo việt minh để thuyết_phục họ khôi_phục lại trật_tự . đêm 18 / 9 / 1945 , a . peter dewey bí_mật gặp trần văn giàu , phạm văn bạch , dương bạch mai , phạm ngọc_thạch và nguyễn văn tạo . tất_cả đều cho rằng quá muộn để thương_lượng và hợp_tác . dân_chúng bị xúc_phạm và kích_động vì thái_độ kiêu_căng của pháp đã ở tư_thế sẵn_sàng làm mọi việc để giữ vững nền độc_lập . trần văn giàu phát_biểu : \"\" hiện_nay , việc cực_kỳ khó_khăn là kiểm_soát được các bè_phái chính_trị khác nhau vì không phải tất_cả thân việt minh mà tất_cả đều chống pháp \"\" .\",Tên gọi nào được Phạm Văn Đồng sử dụng khi làm Phó chủ nhiệm cơ quan Biện sự xứ tại Quế Lâm?,0,train\n",
      "\"cho rằng mình có quyền được đặt tên cho phát_hiện của mình , le verrier ngay lập_tức đề_xuất tên gọi neptune , và tuyên_bố không đúng sự_thực rằng tên gọi này đã được chính_thức công_nhận bởi cơ_quan địa_lý và thiên_văn \"\" bureau des longitudes \"\" của pháp . trong tháng 10 năm 1846 , ông lại đề_nghị sử_dụng tên của chính ông là le verrier , và giám_đốc đài quan_sát françois arago cũng ủng_hộ tên gọi này . tuy_nhiên , đề_nghị này gặp phải sự chống_đối mạnh_mẽ bên ngoài nước pháp . cơ_quan thiên_văn và bản_đồ pháp cũng nhanh_chóng sử_dụng lại tên gọi herschel cho hành_tinh uranus , mang tên người khám_phá sir william herschel , và leverrier cho tên của hành_tinh mới phát_hiện .\",Tên gọi nào được Phạm Văn Đồng sử dụng khi làm Phó chủ nhiệm cơ quan Biện sự xứ tại Quế Lâm?,0,train\n",
      "\"trước thời_điểm cục thủy_văn quốc_tế ( ihb ) , tiền_thân của iho , nhóm_họp hội_nghị quốc_tế đầu_tiên vào ngày 24 tháng 7 năm 1919 , ranh_giới cũng như tên gọi của các biển và đại_dương chưa đạt đồng thuận trên bình diện quốc_tế . sau đó iho đã công_bố những điều này trong tài_liệu giới_hạn của biển và đại_dương ( limits of oceans and seas ) với ấn_bản đầu_tiên năm 1928 . kể từ ấn_bản này , ranh_giới phía bắc của nam đại_dương đã dịch_chuyển dần xuống phía nam ; tới năm 1953 thì nó đã không còn được công_bố chính_thức và các cơ_quan thủy_văn địa_phương được quyền tự_quyết giới_hạn của riêng mình . trong đợt sửa_đổi năm 2000 , iho công_nhận đại_dương này và định_nghĩa nó là vùng nước phía nam vĩ_tuyến 60 ° n , tuy_nhiên điều này lại không chính_thức được thông_qua bởi sự bế_tắc trong những vấn_đề khác như là những tranh_cãi liên_quan tới tên gọi biển nhật bản . dù vậy định_nghĩa của iho năm 2000 đã lưu_hành trong ấn_bản dự_thảo năm 2002 và được một_số tổ_chức trong và ngoài iho sử_dụng ví_dụ như cơ_quan tình_báo trung_ương mỹ , và merriam - webster , một công_ty chuyên xuất_bản từ_điển . [ ct 1 ] giới_chức úc nhận_định vị_trí của nam đại_dương là nằm ngay phía nam nước úc . hiệp_hội địa_lý quốc_gia hoa kỳ thì không công_nhận đại_dương này , họ mô_tả nó ( nếu có ) bằng kiểu chữ khác so với các đại_dương còn lại và thể_hiện thái_bình dương , đại tây_dương và ấn độ dương mở_rộng đến châu nam cực cả trên bản_đồ in và trực_tuyến . hema maps và geonova là hai trong số các nhà xuất_bản bản_đồ có sử_dụng thuật_ngữ nam đại_dương .\",Tên gọi nào được Phạm Văn Đồng sử dụng khi làm Phó chủ nhiệm cơ quan Biện sự xứ tại Quế Lâm?,0,train\n",
      "\"tại miền nam , sự lãnh_đạo của việt minh không vững như tại miền bắc . những người trotskyist đang kiểm_soát ngành cảnh_sát và các tôn_giáo hòa_hảo , cao đài có ác_cảm với việt minh . các giáo_phái được nhật hỗ_trợ phát_triển phong_trào chính_trị của họ mạnh_mẽ . cao đài đông hàng triệu người , tỉnh nào cũng có , họ tập_trung ở sài_gòn đến mấy vạn làm công_nhân và làm binh_lính . lực_lượng cao đài có đảng phục quốc của trần quang_vinh . hòa hảo đông hàng chục vạn người , nhiều nhất là ở hậu_giang , tập_trung tại sài_gòn đến vài ngàn . hòa hảo có chính_đảng là dân xã đảng . giáo_phái tịnh_độ cư_sĩ có hàng vạn quần_chúng , họ không tập_trung lên sài_gòn , nhưng làm cơ_sở quần_chúng cho quốc_gia đảng . phe trotskyist không có đông quần_chúng nhưng một cánh trotskyist là cánh hồ vĩnh ký , huỳnh văn phương cầm_đầu sở mật_thám và sở cảnh_sát nam kỳ tạo thế cho các cánh khác hoạt_động . nhiều tổ_chức khác có năm , bảy trăm , vài_ba ngàn người hợp_tác với sở sen_đầm kempeitai của nhật . đảng_viên bí_mật đảng cộng_sản đông dương phạm ngọc_thạch được thống_đốc nam kỳ minoda fujio và tổng_lãnh_sự nhật ida mời tổ_chức thanh_niên tiền_phong . lực_lượng này chỉ riêng tại sài_gòn đã có hơn 20 vạn người và phát_triển hơn 1 triệu đoàn_viên trong toàn cõi nam kỳ .\",Tên gọi nào được Phạm Văn Đồng sử dụng khi làm Phó chủ nhiệm cơ quan Biện sự xứ tại Quế Lâm?,0,train\n"
     ]
    }
   ],
   "source": [
    "!head train_ranking.csv"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1fccb142",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "7df0a181",
   "metadata": {},
   "source": [
    "## Test Ranking"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "da3b8562",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of the model checkpoint at nguyenvulebinh/vi-mrc-base were not used when initializing RobertaModel: ['qa_outputs.bias', 'qa_outputs.weight']\n",
      "- This IS expected if you are initializing RobertaModel from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n",
      "- This IS NOT expected if you are initializing RobertaModel from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n",
      "Some weights of RobertaModel were not initialized from the model checkpoint at nguyenvulebinh/vi-mrc-base and are newly initialized: ['roberta.pooler.dense.weight', 'roberta.pooler.dense.bias']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "PairwiseModel(\n",
       "  (model): RobertaModel(\n",
       "    (embeddings): RobertaEmbeddings(\n",
       "      (word_embeddings): Embedding(250002, 768, padding_idx=1)\n",
       "      (position_embeddings): Embedding(514, 768, padding_idx=1)\n",
       "      (token_type_embeddings): Embedding(1, 768)\n",
       "      (LayerNorm): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
       "      (dropout): Dropout(p=0.1, inplace=False)\n",
       "    )\n",
       "    (encoder): RobertaEncoder(\n",
       "      (layer): ModuleList(\n",
       "        (0): RobertaLayer(\n",
       "          (attention): RobertaAttention(\n",
       "            (self): RobertaSelfAttention(\n",
       "              (query): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (key): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (value): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (dropout): Dropout(p=0.1, inplace=False)\n",
       "            )\n",
       "            (output): RobertaSelfOutput(\n",
       "              (dense): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (LayerNorm): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
       "              (dropout): Dropout(p=0.1, inplace=False)\n",
       "            )\n",
       "          )\n",
       "          (intermediate): RobertaIntermediate(\n",
       "            (dense): Linear(in_features=768, out_features=3072, bias=True)\n",
       "            (intermediate_act_fn): GELUActivation()\n",
       "          )\n",
       "          (output): RobertaOutput(\n",
       "            (dense): Linear(in_features=3072, out_features=768, bias=True)\n",
       "            (LayerNorm): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
       "            (dropout): Dropout(p=0.1, inplace=False)\n",
       "          )\n",
       "        )\n",
       "        (1): RobertaLayer(\n",
       "          (attention): RobertaAttention(\n",
       "            (self): RobertaSelfAttention(\n",
       "              (query): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (key): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (value): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (dropout): Dropout(p=0.1, inplace=False)\n",
       "            )\n",
       "            (output): RobertaSelfOutput(\n",
       "              (dense): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (LayerNorm): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
       "              (dropout): Dropout(p=0.1, inplace=False)\n",
       "            )\n",
       "          )\n",
       "          (intermediate): RobertaIntermediate(\n",
       "            (dense): Linear(in_features=768, out_features=3072, bias=True)\n",
       "            (intermediate_act_fn): GELUActivation()\n",
       "          )\n",
       "          (output): RobertaOutput(\n",
       "            (dense): Linear(in_features=3072, out_features=768, bias=True)\n",
       "            (LayerNorm): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
       "            (dropout): Dropout(p=0.1, inplace=False)\n",
       "          )\n",
       "        )\n",
       "        (2): RobertaLayer(\n",
       "          (attention): RobertaAttention(\n",
       "            (self): RobertaSelfAttention(\n",
       "              (query): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (key): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (value): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (dropout): Dropout(p=0.1, inplace=False)\n",
       "            )\n",
       "            (output): RobertaSelfOutput(\n",
       "              (dense): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (LayerNorm): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
       "              (dropout): Dropout(p=0.1, inplace=False)\n",
       "            )\n",
       "          )\n",
       "          (intermediate): RobertaIntermediate(\n",
       "            (dense): Linear(in_features=768, out_features=3072, bias=True)\n",
       "            (intermediate_act_fn): GELUActivation()\n",
       "          )\n",
       "          (output): RobertaOutput(\n",
       "            (dense): Linear(in_features=3072, out_features=768, bias=True)\n",
       "            (LayerNorm): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
       "            (dropout): Dropout(p=0.1, inplace=False)\n",
       "          )\n",
       "        )\n",
       "        (3): RobertaLayer(\n",
       "          (attention): RobertaAttention(\n",
       "            (self): RobertaSelfAttention(\n",
       "              (query): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (key): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (value): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (dropout): Dropout(p=0.1, inplace=False)\n",
       "            )\n",
       "            (output): RobertaSelfOutput(\n",
       "              (dense): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (LayerNorm): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
       "              (dropout): Dropout(p=0.1, inplace=False)\n",
       "            )\n",
       "          )\n",
       "          (intermediate): RobertaIntermediate(\n",
       "            (dense): Linear(in_features=768, out_features=3072, bias=True)\n",
       "            (intermediate_act_fn): GELUActivation()\n",
       "          )\n",
       "          (output): RobertaOutput(\n",
       "            (dense): Linear(in_features=3072, out_features=768, bias=True)\n",
       "            (LayerNorm): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
       "            (dropout): Dropout(p=0.1, inplace=False)\n",
       "          )\n",
       "        )\n",
       "        (4): RobertaLayer(\n",
       "          (attention): RobertaAttention(\n",
       "            (self): RobertaSelfAttention(\n",
       "              (query): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (key): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (value): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (dropout): Dropout(p=0.1, inplace=False)\n",
       "            )\n",
       "            (output): RobertaSelfOutput(\n",
       "              (dense): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (LayerNorm): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
       "              (dropout): Dropout(p=0.1, inplace=False)\n",
       "            )\n",
       "          )\n",
       "          (intermediate): RobertaIntermediate(\n",
       "            (dense): Linear(in_features=768, out_features=3072, bias=True)\n",
       "            (intermediate_act_fn): GELUActivation()\n",
       "          )\n",
       "          (output): RobertaOutput(\n",
       "            (dense): Linear(in_features=3072, out_features=768, bias=True)\n",
       "            (LayerNorm): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
       "            (dropout): Dropout(p=0.1, inplace=False)\n",
       "          )\n",
       "        )\n",
       "        (5): RobertaLayer(\n",
       "          (attention): RobertaAttention(\n",
       "            (self): RobertaSelfAttention(\n",
       "              (query): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (key): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (value): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (dropout): Dropout(p=0.1, inplace=False)\n",
       "            )\n",
       "            (output): RobertaSelfOutput(\n",
       "              (dense): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (LayerNorm): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
       "              (dropout): Dropout(p=0.1, inplace=False)\n",
       "            )\n",
       "          )\n",
       "          (intermediate): RobertaIntermediate(\n",
       "            (dense): Linear(in_features=768, out_features=3072, bias=True)\n",
       "            (intermediate_act_fn): GELUActivation()\n",
       "          )\n",
       "          (output): RobertaOutput(\n",
       "            (dense): Linear(in_features=3072, out_features=768, bias=True)\n",
       "            (LayerNorm): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
       "            (dropout): Dropout(p=0.1, inplace=False)\n",
       "          )\n",
       "        )\n",
       "        (6): RobertaLayer(\n",
       "          (attention): RobertaAttention(\n",
       "            (self): RobertaSelfAttention(\n",
       "              (query): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (key): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (value): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (dropout): Dropout(p=0.1, inplace=False)\n",
       "            )\n",
       "            (output): RobertaSelfOutput(\n",
       "              (dense): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (LayerNorm): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
       "              (dropout): Dropout(p=0.1, inplace=False)\n",
       "            )\n",
       "          )\n",
       "          (intermediate): RobertaIntermediate(\n",
       "            (dense): Linear(in_features=768, out_features=3072, bias=True)\n",
       "            (intermediate_act_fn): GELUActivation()\n",
       "          )\n",
       "          (output): RobertaOutput(\n",
       "            (dense): Linear(in_features=3072, out_features=768, bias=True)\n",
       "            (LayerNorm): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
       "            (dropout): Dropout(p=0.1, inplace=False)\n",
       "          )\n",
       "        )\n",
       "        (7): RobertaLayer(\n",
       "          (attention): RobertaAttention(\n",
       "            (self): RobertaSelfAttention(\n",
       "              (query): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (key): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (value): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (dropout): Dropout(p=0.1, inplace=False)\n",
       "            )\n",
       "            (output): RobertaSelfOutput(\n",
       "              (dense): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (LayerNorm): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
       "              (dropout): Dropout(p=0.1, inplace=False)\n",
       "            )\n",
       "          )\n",
       "          (intermediate): RobertaIntermediate(\n",
       "            (dense): Linear(in_features=768, out_features=3072, bias=True)\n",
       "            (intermediate_act_fn): GELUActivation()\n",
       "          )\n",
       "          (output): RobertaOutput(\n",
       "            (dense): Linear(in_features=3072, out_features=768, bias=True)\n",
       "            (LayerNorm): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
       "            (dropout): Dropout(p=0.1, inplace=False)\n",
       "          )\n",
       "        )\n",
       "        (8): RobertaLayer(\n",
       "          (attention): RobertaAttention(\n",
       "            (self): RobertaSelfAttention(\n",
       "              (query): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (key): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (value): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (dropout): Dropout(p=0.1, inplace=False)\n",
       "            )\n",
       "            (output): RobertaSelfOutput(\n",
       "              (dense): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (LayerNorm): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
       "              (dropout): Dropout(p=0.1, inplace=False)\n",
       "            )\n",
       "          )\n",
       "          (intermediate): RobertaIntermediate(\n",
       "            (dense): Linear(in_features=768, out_features=3072, bias=True)\n",
       "            (intermediate_act_fn): GELUActivation()\n",
       "          )\n",
       "          (output): RobertaOutput(\n",
       "            (dense): Linear(in_features=3072, out_features=768, bias=True)\n",
       "            (LayerNorm): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
       "            (dropout): Dropout(p=0.1, inplace=False)\n",
       "          )\n",
       "        )\n",
       "        (9): RobertaLayer(\n",
       "          (attention): RobertaAttention(\n",
       "            (self): RobertaSelfAttention(\n",
       "              (query): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (key): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (value): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (dropout): Dropout(p=0.1, inplace=False)\n",
       "            )\n",
       "            (output): RobertaSelfOutput(\n",
       "              (dense): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (LayerNorm): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
       "              (dropout): Dropout(p=0.1, inplace=False)\n",
       "            )\n",
       "          )\n",
       "          (intermediate): RobertaIntermediate(\n",
       "            (dense): Linear(in_features=768, out_features=3072, bias=True)\n",
       "            (intermediate_act_fn): GELUActivation()\n",
       "          )\n",
       "          (output): RobertaOutput(\n",
       "            (dense): Linear(in_features=3072, out_features=768, bias=True)\n",
       "            (LayerNorm): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
       "            (dropout): Dropout(p=0.1, inplace=False)\n",
       "          )\n",
       "        )\n",
       "        (10): RobertaLayer(\n",
       "          (attention): RobertaAttention(\n",
       "            (self): RobertaSelfAttention(\n",
       "              (query): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (key): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (value): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (dropout): Dropout(p=0.1, inplace=False)\n",
       "            )\n",
       "            (output): RobertaSelfOutput(\n",
       "              (dense): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (LayerNorm): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
       "              (dropout): Dropout(p=0.1, inplace=False)\n",
       "            )\n",
       "          )\n",
       "          (intermediate): RobertaIntermediate(\n",
       "            (dense): Linear(in_features=768, out_features=3072, bias=True)\n",
       "            (intermediate_act_fn): GELUActivation()\n",
       "          )\n",
       "          (output): RobertaOutput(\n",
       "            (dense): Linear(in_features=3072, out_features=768, bias=True)\n",
       "            (LayerNorm): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
       "            (dropout): Dropout(p=0.1, inplace=False)\n",
       "          )\n",
       "        )\n",
       "        (11): RobertaLayer(\n",
       "          (attention): RobertaAttention(\n",
       "            (self): RobertaSelfAttention(\n",
       "              (query): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (key): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (value): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (dropout): Dropout(p=0.1, inplace=False)\n",
       "            )\n",
       "            (output): RobertaSelfOutput(\n",
       "              (dense): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (LayerNorm): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
       "              (dropout): Dropout(p=0.1, inplace=False)\n",
       "            )\n",
       "          )\n",
       "          (intermediate): RobertaIntermediate(\n",
       "            (dense): Linear(in_features=768, out_features=3072, bias=True)\n",
       "            (intermediate_act_fn): GELUActivation()\n",
       "          )\n",
       "          (output): RobertaOutput(\n",
       "            (dense): Linear(in_features=3072, out_features=768, bias=True)\n",
       "            (LayerNorm): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
       "            (dropout): Dropout(p=0.1, inplace=False)\n",
       "          )\n",
       "        )\n",
       "      )\n",
       "    )\n",
       "    (pooler): RobertaPooler(\n",
       "      (dense): Linear(in_features=768, out_features=768, bias=True)\n",
       "      (activation): Tanh()\n",
       "    )\n",
       "  )\n",
       "  (fc): Linear(in_features=768, out_features=1, bias=True)\n",
       ")"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from model import *\n",
    "pairwise_model_stage1 = PairwiseModel(\"nguyenvulebinh/vi-mrc-base\", )\n",
    "pairwise_model_stage1.load_state_dict(torch.load(\"pairwise_v2.bin\"))\n",
    "pairwise_model_stage1.eval()\n",
    "pairwise_model_stage1.to('cuda')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "270be436",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Reranking with pairwise model for top10\n",
    "def rerank(question, contexts):\n",
    "    ranking_preds = pairwise_model_stage1.stage1_ranking(question, contexts)\n",
    "    ranking_scores = ranking_preds \n",
    "\n",
    "    #Question answering\n",
    "    best_idxs = np.argsort(ranking_scores)[-10:]\n",
    "    ranking_scores = np.array(ranking_scores)[best_idxs]\n",
    "    texts = np.array(contexts)[best_idxs]\n",
    "    return texts "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "896eea75",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
